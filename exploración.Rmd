---
title: "Exploración"
author: "Miguel Pérez Barrero"
date: "1/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## TFM

### Librerías y dependencias

```{r Librerías}
library(tidyr)
library(tibble)
library(dplyr)
library(ggplot2)
library(patchwork)
library(visdat)
library(data.table)
library(graphics)
library(reticulate)
library(lubridate)
```

### Carga de datos

```{r Carga de datos}
base_dir <- '../../datasets/SocialEvolution/'

calls <- read.csv2(paste0(base_dir, 'Calls.csv'), sep = ',')
sms <- read.csv2(paste0(base_dir, 'SMS.csv'), sep = ',')
proximity <- read.csv2(paste0(base_dir, 'Proximity.csv'), sep = ',')
wlan <- read.csv2(paste0(base_dir, 'WLAN2.csv'), sep = ',')
relationships <- read.csv2(paste0(base_dir, 'RelationshipsFromSurveys.csv'), sep = ',')
activities <- read.csv2(paste0(base_dir, 'Activities.csv'), sep = ',')
flu <- read.csv2(paste0(base_dir, 'FluSymptoms.csv'), sep = ',')
health <- read.csv2(paste0(base_dir, 'Health.csv'), sep = ',')
politics <- read.csv2(paste0(base_dir, 'Politics.csv'), sep = ',')
music_aware <- read.csv2(paste0(base_dir, 'MusicGenreAwareness.csv'), sep = ',')
music_immersion <- read.csv2(paste0(base_dir, 'MusicGenreImmersion.csv'), sep = ',')
music_pref <- read.csv2(paste0(base_dir, 'MusicGenrePreference.csv'), sep = ',')
subjects <- read.csv2(paste0(base_dir, 'Subjects.csv'), sep = ',')
```


### Limpieza de los datos

```{r Vista de pájaro}
catalog <- c('calls','sms','proximity','wlan','relationships','activities','flu','health','politics','music_aware','music_immersion','music_pref','subjects')
datasets = list(calls,sms,proximity,wlan,relationships,activities,flu,health,politics,music_aware,music_immersion,music_pref,subjects)

lapply(seq_along(datasets), function(index){vis_dat(datasets[[index]], warn_large_data = FALSE) + labs(title = catalog[index])})
```

Todos los datasets están bastante completos excepto unos pocos que tienen falta:
  health: no tiene muchos datos de peso y alguna línea casi sin datos.
  calls: faltan muchos destinatarios y algún que otro origen
  sms: faltan muchos destinatarios

### Teoría de la ubicación

El objetivo es construir ubicaciones a partir de las MACs de las LANs.

1. En un momento dado un usuario está conectado a N LANs con diferente intensidad.
  Estas LANs tendrían relación de vecindad.

```{r Vecindad de LANs}
grouped <- wlan %>% group_by(user_id, time) %>% summarise(total = n(), ids=paste(wireless_mac, collapse = ","), .groups = "keep")
```
Se pueden separar las macs mas importantes por ser las que aparecen en el registro con más de un usuario en un rango de tiempo determinado.

En un primer escaneo miramos cuantos usuarios hay en el mismo instante de tiempo
```{r Cuenta usuarios simultáneos}
hour <- 3600

count_events <- wlan %>% 
  mutate(cell = unix_time %/% (hour/6)) %>%
  group_by(wireless_mac, user_id, cell) %>% 
  summarise(total = n(), .groups = "keep") %>%
  group_by(total) %>% summarise(count = n(), .groups = "keep")
pie(count_events$count)
```

Miramos que wifis son más importantes, porque tienen la mayoría de los eventos de los usuarios que traceamos
El criterio será: las wifis que son visitadas por al menos 3 de nuestros usuarios objetivo en intervalos de 10 minutos.

```{r}
relevant_wifis <- wlan %>% 
  mutate(cell = unix_time %/% (hour/6)) %>%
  group_by(wireless_mac, user_id, cell) %>% 
  summarise(total = n(), .groups = "keep") %>%
  filter(total > 3)
macs <- unique(relevant_wifis$wireless_mac)
```

Usuarios originales

```{r}
orig_users <- sort(unique(wlan$user_id))
orig_users
```

Usuarios que se mantienen restringiendo los eventos a las wifis relevantes

```{r}
filtered_wlan_events <- wlan %>% filter(wireless_mac %in% macs)
orig_users == sort(unique(filtered_wlan_events$user_id))
```


Una vez eliminadas las wifis poco relevantes, intentamos clusterizar el resto. Los clusters que obtengamos los consideramos zonas de incidencia.
La idea es una especie de teselación.Se puede intentar agrupar las wifis que significan una misma area o agrupar los eventos en áreas cuando tienen la misma distancia a las mismas (o similares) wifis.

```{r}
hist(filtered_wlan_events$strength)
```

```{r}
length(macs)
```

Ahora tenemos 37 áreas de interés. La idea es clusterizarlas para manejar un número razonable de ellas.

Opciones que se me ocurren son:
  - Escalamiento multidimensional para asignar un (x,y) a las MACs y con ello luego clusterizar por distancia.
  - 
  
Después obtener una "traza" del usuario tomando unos intervalos de tiempo definidos. Esta sería la traza de ubicaciones. De otras tablas podemos obtener la traza de contactos. 

El propio artículo dice que no se puede obtener la ubicación precisa con los datos que tengo. Otra estrategia es intentar clusterizar los datos directamente, de tal manera que se vea que los usuarios cuando están "en el mismo sitio" tienen unos eventos de conexión parecidos a "las mismas wifis".

Es mejor usar las 37 wifis como dimensiones, de tal modo que, en un mismo bloque de tiempo, 10 minutos por ejemplo, cada individuo se define por un vector compuesto por las intensidades a cada una de las 36 wifis. De esta forma se puede aplicar reducción de dimensión (Componentes principales) y clustering después para determinar las "zonas".

Y es razonable decir que los puntos que están a distancias parecidas de las mismas wifis están "cerca".

La forma de trabajar con el tiempo aquí es usar cada posición / registro como una muestra más del "mapa" de wifis. Es decir, como un punto más, no como puntos que se mueven.
Posteriormente, cada posición se asociará al usuario para saber donde estaba en ese momento.

Es decir, la estrategia es:
* Clusterizar el conjunto de posiciones de la tabla de posiciones respecto a wifis.
  * El identificador de posición está compuesto por el id de usuario + timestamp.
* Una vez clusterizado, ver a que región corresponde la posición de un usuario en un tiempo determinado.
* Generar una tabla con las posiciones en los últimos N intervalos de tiempo.

```{r}
# gemerar la tabla de posiciones para hacer clustering, se crea un id para relacionar luego con el tiempo, que no
# es relevante para el clusterizado
positions_wifi_space <- data.frame(filtered_wlan_events) %>% 
                            mutate(position_id = paste0(user_id,":",unix_time)) %>% 
                            select(position_id, wireless_mac, strength, time, unix_time, user_id) %>%
                            filter(strength != 0) %>% # eliminamos las wifis que pudieran tener cero señal
                            pivot_wider(names_from = wireless_mac, values_from = strength, values_fill = 0)
write.csv(positions_wifi_space, "positions_wifi_space.csv", row.names = FALSE)


```

Como tenemos muchas dimensiones, vamos a intentar reducir dimensiones con el algoritmo de componentes princiales

```{r}
require(graphics)
comp <- prcomp(positions_wifi_space[, -1], scale = TRUE)
plot(comp)
abline(h = 1)
```

Deberíamos considerar unas 6 dimensiones (varianza mayor que 1)

```{r}
biplot(comp)
```

```{r}
summary(comp)
```

Vemos el algoritmo de clusterización sobre los datos rotados

```{r}
saveRDS(comp, "comp.RData")
datos_rotados <- data.frame(comp$x[,c(1,2,3,4,5,6,7,8)])
datos_rotados$position_id = positions_wifi_space$position_id
write.csv(datos_rotados, "datos_rotados.csv", row.names = FALSE)
```

### Datos de sintomas de gripe

```{r}
summary(flu)
class(flu)
```

Buscamos si hay registros duplicados

```{r}
duplicates <- flu %>% 
              distinct(user_id, time, fever) %>%
              select(user_id, time, fever) %>% 
              group_by(user_id, time, fever) %>% 
              summarize(count = n(), .groups='keep')
duplicates <- duplicates %>% filter(count > 1)
```

Eliminamos los duplicados y vemos la evolución de un sintoma para todos los usuarios a lo largo del tiempo:

```{r}
z <- flu %>% distinct(user_id, time, fever) %>% select(user_id, time, fever)
image(as.matrix(z))
```

Va a haber ciertos problemas para poner estos datos en modo matriz perfecto, porque los indices
de tiempo no son iguales entre todos los usuarios, y hay que "rellenar" en los usuarios que no
tienen muestra en un momento dado.

Por así decirlo, uno de los problemas es crear una rejilla temporal consistente para todos los eventos
que estamos analizando: posición-wifi, sintomas gripe, etc.

También el problema en este caso es "crear los ceros", es decir, poner los valores nulos en la rejilla donde
ahora no tenemos datos.


